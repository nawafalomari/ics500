{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompter.random_prompter import RandomPrompter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cort_feature_envy_train_df = pd.read_csv('CoRT/Long Method/long_method_df_train.csv')\n",
    "cort_feature_envy_test_df = pd.read_csv('CoRT/Long Method/long_method_df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Smelly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>public void preStore ( Object parameter ) thr...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Code Smelly\n",
       "543   public void preStore ( Object parameter ) thr...     No"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cort_feature_envy_train_df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename Code column to Text\n",
    "\n",
    "cort_feature_envy_train_df.rename(columns = {'Code':'text'}, inplace = True)\n",
    "cort_feature_envy_test_df.rename(columns = {'Code':'text'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "from time import sleep\n",
    "\n",
    "def get_gpt_completion(prompt):\n",
    "    sleep(1)\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Only give one best answer without any explanation. The answer should be form the choices given only.\"\n",
    "                    }\n",
    "                ]\n",
    "                },\n",
    "                {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\":  prompt\n",
    "                    }\n",
    "                ]\n",
    "                }\n",
    "            ],\n",
    "            temperature=1,\n",
    "            max_tokens=2048,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            response_format={\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    ")\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"In terms of Long Method Smell, the following method can be classified as Smelly if it contains Long Method or No if it does not contain the smell. Only answer with the word Smelly or the word No.\\nThe code sample:{{INPUT}}\\nThe best classified as: {{TARGET}}\"\n",
    "\n",
    "random_prompter = RandomPrompter(prompt, get_completion=get_gpt_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_prompter.fit(X=cort_feature_envy_train_df['text'], y=cort_feature_envy_train_df['Smelly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting classes for the test set...\n",
      "[=====================] 99% sample 299 / 300 Done predicting classes for the test set, you can call evaluate method to get the results\n"
     ]
    }
   ],
   "source": [
    "random_prompter.test(X_test=cort_feature_envy_test_df['text'], y_test=cort_feature_envy_test_df['Smelly'], n_shots=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.87      0.63      0.73       150\n",
      "      Smelly       0.71      0.91      0.80       150\n",
      "\n",
      "    accuracy                           0.77       300\n",
      "   macro avg       0.79      0.77      0.76       300\n",
      "weighted avg       0.79      0.77      0.76       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_prompter.flush_blocked()\n",
    "random_prompter.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "y_true =  random_prompter.test_df[\"class\"]\n",
    "y_pred =  random_prompter.test_df[\"preds\"]\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting classes for the test set...\n",
      "[=====================] 99% sample 299 / 300 Done predicting classes for the test set, you can call evaluate method to get the results\n"
     ]
    }
   ],
   "source": [
    "random_prompter.test(X_test=cort_feature_envy_test_df['text'], y_test=cort_feature_envy_test_df['Smelly'], n_shots=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_prompter.flush_blocked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.64      0.76      0.70       150\n",
      "      Smelly       0.70      0.57      0.63       150\n",
      "\n",
      "    accuracy                           0.67       300\n",
      "   macro avg       0.67      0.67      0.66       300\n",
      "weighted avg       0.67      0.67      0.66       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_prompter.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true =  random_prompter.test_df[\"class\"]\n",
    "y_pred =  random_prompter.test_df[\"preds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.3392970532287465\n"
     ]
    }
   ],
   "source": [
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting classes for the test set...\n",
      "[=====================] 99% sample 299 / 300 Done predicting classes for the test set, you can call evaluate method to get the results\n"
     ]
    }
   ],
   "source": [
    "random_prompter.test(X_test=cort_feature_envy_test_df['text'], y_test=cort_feature_envy_test_df['Smelly'], n_shots=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.64      0.84      0.73       150\n",
      "      Smelly       0.77      0.53      0.63       150\n",
      "\n",
      "    accuracy                           0.69       300\n",
      "   macro avg       0.71      0.69      0.68       300\n",
      "weighted avg       0.71      0.69      0.68       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_prompter.flush_blocked()\n",
    "random_prompter.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.3922322702763681\n"
     ]
    }
   ],
   "source": [
    "y_true =  random_prompter.test_df[\"class\"]\n",
    "y_pred =  random_prompter.test_df[\"preds\"]\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting classes for the test set...\n",
      "[=====================] 99% sample 299 / 300 Done predicting classes for the test set, you can call evaluate method to get the results\n"
     ]
    }
   ],
   "source": [
    "random_prompter.test(X_test=cort_feature_envy_test_df['text'], y_test=cort_feature_envy_test_df['Smelly'], n_shots=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.66      0.85      0.74       150\n",
      "      Smelly       0.79      0.56      0.66       150\n",
      "\n",
      "    accuracy                           0.71       300\n",
      "   macro avg       0.73      0.71      0.70       300\n",
      "weighted avg       0.73      0.71      0.70       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_prompter.flush_blocked()\n",
    "\n",
    "random_prompter.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.43235241772226723\n"
     ]
    }
   ],
   "source": [
    "y_true =  random_prompter.test_df[\"class\"]\n",
    "y_pred =  random_prompter.test_df[\"preds\"]\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
