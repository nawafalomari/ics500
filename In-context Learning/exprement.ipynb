{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompter.random_prompter import RandomPrompter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cort_feature_envy_train_df = pd.read_csv('CoRT/Feature Envy/feature_envy_df_train.csv')\n",
    "cort_feature_envy_test_df = pd.read_csv('CoRT/Feature Envy/feature_envy_df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Smelly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>public void initialize ( ) throws Instantiati...</td>\n",
       "      <td>Smelly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Code  Smelly\n",
       "845   public void initialize ( ) throws Instantiati...  Smelly"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cort_feature_envy_train_df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename Code column to Text\n",
    "\n",
    "cort_feature_envy_train_df.rename(columns = {'Code':'text'}, inplace = True)\n",
    "cort_feature_envy_test_df.rename(columns = {'Code':'text'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_gpt_completion(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Only give one best answer without any explanation. The answer should be form the choices given only.\"\n",
    "                    }\n",
    "                ]\n",
    "                },\n",
    "                {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\":  prompt\n",
    "                    }\n",
    "                ]\n",
    "                }\n",
    "            ],\n",
    "            temperature=1,\n",
    "            max_tokens=2048,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            response_format={\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    ")\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"In terms of Feature Envy Smell, the following method can be classified as Smelly if it contains Feature Envy or No if it does not contain the smell. Only answer with the word Smelly or the word No.\\nThe code sample:{{INPUT}}\\nThe best classified as: {{TARGET}}\"\n",
    "\n",
    "random_prompter = RandomPrompter(prompt, get_completion=get_gpt_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_prompter.fit(X=cort_feature_envy_train_df['text'], y=cort_feature_envy_train_df['Smelly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting classes for the test set...\n",
      "[=====================] 99% sample 299 / 300 Done predicting classes for the test set, you can call evaluate method to get the results\n"
     ]
    }
   ],
   "source": [
    "random_prompter.test(X_test=cort_feature_envy_test_df['text'], y_test=cort_feature_envy_test_df['Smelly'], n_shots=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.74      0.73      0.73       150\n",
      "      Smelly       0.73      0.75      0.74       150\n",
      "\n",
      "    accuracy                           0.74       300\n",
      "   macro avg       0.74      0.74      0.74       300\n",
      "weighted avg       0.74      0.74      0.74       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_prompter.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.47342802840946996\n"
     ]
    }
   ],
   "source": [
    "y_true =  random_prompter.test_df[\"class\"]\n",
    "y_pred =  random_prompter.test_df[\"preds\"]\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting classes for the test set...\n",
      "[=====================] 99% sample 299 / 300 Done predicting classes for the test set, you can call evaluate method to get the results\n"
     ]
    }
   ],
   "source": [
    "random_prompter.test(X_test=cort_feature_envy_test_df['text'], y_test=cort_feature_envy_test_df['Smelly'], n_shots=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.65      0.77      0.70       150\n",
      "      Smelly       0.71      0.58      0.64       150\n",
      "\n",
      "    accuracy                           0.67       300\n",
      "   macro avg       0.68      0.67      0.67       300\n",
      "weighted avg       0.68      0.67      0.67       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_prompter.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true =  random_prompter.test_df[\"class\"]\n",
    "y_pred =  random_prompter.test_df[\"preds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.3528689353578963\n"
     ]
    }
   ],
   "source": [
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting classes for the test set...\n",
      "[=====================] 99% sample 299 / 300 Done predicting classes for the test set, you can call evaluate method to get the results\n"
     ]
    }
   ],
   "source": [
    "random_prompter.test(X_test=cort_feature_envy_test_df['text'], y_test=cort_feature_envy_test_df['Smelly'], n_shots=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.66      0.80      0.73       150\n",
      "      Smelly       0.75      0.59      0.66       150\n",
      "\n",
      "    accuracy                           0.70       300\n",
      "   macro avg       0.71      0.70      0.69       300\n",
      "weighted avg       0.71      0.70      0.69       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_prompter.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.4020122097616405\n"
     ]
    }
   ],
   "source": [
    "y_true =  random_prompter.test_df[\"class\"]\n",
    "y_pred =  random_prompter.test_df[\"preds\"]\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting classes for the test set...\n",
      "[=====================] 99% sample 299 / 300 Done predicting classes for the test set, you can call evaluate method to get the results\n"
     ]
    }
   ],
   "source": [
    "random_prompter.test(X_test=cort_feature_envy_test_df['text'], y_test=cort_feature_envy_test_df['Smelly'], n_shots=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.67      0.77      0.71       150\n",
      "      Smelly       0.73      0.62      0.67       150\n",
      "\n",
      "    accuracy                           0.69       300\n",
      "   macro avg       0.70      0.69      0.69       300\n",
      "weighted avg       0.70      0.69      0.69       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_prompter.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.3908938028624642\n"
     ]
    }
   ],
   "source": [
    "y_true =  random_prompter.test_df[\"class\"]\n",
    "y_pred =  random_prompter.test_df[\"preds\"]\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
