{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompter.random_prompter import RandomPrompter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cort_feature_envy_train_df = pd.read_csv('CoRT/God Class/god_class_df_train.csv')\n",
    "cort_feature_envy_test_df = pd.read_csv('CoRT/God Class/god_class_df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Smelly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>public class AnalyzerAdapter extends MethodVis...</td>\n",
       "      <td>Smelly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Code  Smelly\n",
       "1041  public class AnalyzerAdapter extends MethodVis...  Smelly"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cort_feature_envy_train_df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename Code column to Text\n",
    "\n",
    "cort_feature_envy_train_df.rename(columns = {'Code':'text'}, inplace = True)\n",
    "cort_feature_envy_test_df.rename(columns = {'Code':'text'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "from time import sleep\n",
    "\n",
    "def get_gpt_completion(prompt):\n",
    "    sleep(1)\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Only give one best answer without any explanation. The answer should be form the choices given only.\"\n",
    "                    }\n",
    "                ]\n",
    "                },\n",
    "                {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\":  prompt\n",
    "                    }\n",
    "                ]\n",
    "                }\n",
    "            ],\n",
    "            temperature=1,\n",
    "            max_tokens=2048,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            response_format={\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    ")\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"In terms of God Class Smell, the following method can be classified as Smelly if it contains God Class or No if it does not contain the smell. Only answer with the word Smelly or the word No.\\nThe code sample:{{INPUT}}\\nThe best classified as: {{TARGET}}\"\n",
    "\n",
    "random_prompter = RandomPrompter(prompt, get_completion=get_gpt_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_prompter.fit(X=cort_feature_envy_train_df['text'], y=cort_feature_envy_train_df['Smelly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting classes for the test set...\n",
      "[=====================] 99% sample 299 / 300 Done predicting classes for the test set, you can call evaluate method to get the results\n"
     ]
    }
   ],
   "source": [
    "random_prompter.test(X_test=cort_feature_envy_test_df['text'], y_test=cort_feature_envy_test_df['Smelly'], n_shots=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.82      0.37      0.51       150\n",
      "      Smelly       0.59      0.92      0.72       150\n",
      "\n",
      "    accuracy                           0.64       300\n",
      "   macro avg       0.71      0.64      0.61       300\n",
      "weighted avg       0.71      0.64      0.61       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_prompter.flush_blocked()\n",
    "random_prompter.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.3441542156406519\n"
     ]
    }
   ],
   "source": [
    "y_true =  random_prompter.test_df[\"class\"]\n",
    "y_pred =  random_prompter.test_df[\"preds\"]\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting classes for the test set...\n",
      "[=====================] 99% sample 299 / 300 Done predicting classes for the test set, you can call evaluate method to get the results\n"
     ]
    }
   ],
   "source": [
    "random_prompter.test(X_test=cort_feature_envy_test_df['text'], y_test=cort_feature_envy_test_df['Smelly'], n_shots=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_prompter.flush_blocked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.74      0.69      0.71       150\n",
      "      Smelly       0.71      0.75      0.73       150\n",
      "\n",
      "    accuracy                           0.72       300\n",
      "   macro avg       0.72      0.72      0.72       300\n",
      "weighted avg       0.72      0.72      0.72       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_prompter.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true =  random_prompter.test_df[\"class\"]\n",
    "y_pred =  random_prompter.test_df[\"preds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.44098104915550024\n"
     ]
    }
   ],
   "source": [
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting classes for the test set...\n",
      "[=======             ] 31% sample 95 / 300 Error in completion for sample  95\n",
      "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o-mini in organization org-ytVeK9xpa0LZyo8tAQgvK6UR on tokens per min (TPM): Limit 200000, Requested 500022. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "[=====================] 99% sample 299 / 300 Done predicting classes for the test set, you can call evaluate method to get the results\n"
     ]
    }
   ],
   "source": [
    "random_prompter.test(X_test=cort_feature_envy_test_df['text'], y_test=cort_feature_envy_test_df['Smelly'], n_shots=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.79      0.63      0.70       150\n",
      "      Smelly       0.69      0.83      0.76       150\n",
      "\n",
      "    accuracy                           0.73       300\n",
      "   macro avg       0.74      0.73      0.73       300\n",
      "weighted avg       0.74      0.73      0.73       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_prompter.flush_blocked()\n",
    "random_prompter.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.47014987243310497\n"
     ]
    }
   ],
   "source": [
    "y_true =  random_prompter.test_df[\"class\"]\n",
    "y_pred =  random_prompter.test_df[\"preds\"]\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting classes for the test set...\n",
      "[========            ] 39% sample 117 / 300 Error in completion for sample  117\n",
      "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o-mini in organization org-ytVeK9xpa0LZyo8tAQgvK6UR on tokens per min (TPM): Limit 200000, Requested 489015. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "[=====================] 98% sample 294 / 300 Error in completion for sample  294\n",
      "Error code: 429 - {'error': {'message': 'Request too large for gpt-4o-mini in organization org-ytVeK9xpa0LZyo8tAQgvK6UR on tokens per min (TPM): Limit 200000, Requested 487365. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "[=====================] 99% sample 299 / 300 Done predicting classes for the test set, you can call evaluate method to get the results\n"
     ]
    }
   ],
   "source": [
    "random_prompter.test(X_test=cort_feature_envy_test_df['text'], y_test=cort_feature_envy_test_df['Smelly'], n_shots=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.83      0.61      0.70       150\n",
      "      Smelly       0.69      0.87      0.77       150\n",
      "\n",
      "    accuracy                           0.74       300\n",
      "   macro avg       0.76      0.74      0.74       300\n",
      "weighted avg       0.76      0.74      0.74       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_prompter.flush_blocked()\n",
    "\n",
    "random_prompter.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.49803441398357373\n"
     ]
    }
   ],
   "source": [
    "y_true =  random_prompter.test_df[\"class\"]\n",
    "y_pred =  random_prompter.test_df[\"preds\"]\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
